#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Smart Assistant - –£–ª—É—á—à–µ–Ω–Ω–∞—è RAG-—Å–∏—Å—Ç–µ–º–∞ —Å –æ–±—É—á–µ–Ω–∏–µ–º –Ω–∞ OpenAI
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π, –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ –∫–ª—É–±–µ
"""

import os
import sqlite3
import logging
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import openai

logger = logging.getLogger(__name__)


class SmartAssistant:
    """
    –£–º–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π RAG-—Å–∏—Å—Ç–µ–º–æ–π
    - –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç .md —Ñ–∞–π–ª—ã (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –ø—Ä–∞–≤–∏–ª–∞)
    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π
    - –ö–æ–Ω—Ç–µ–∫—Å—Ç –æ –∫–ª—É–±–µ –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞—Ö
    - –†–∞–∑–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤
    """

    def __init__(self,
                 kb,  # KnowledgeBase instance
                 embedding_service,  # EmbeddingService instance
                 vector_store,  # VectorStore instance
                 db_path: str,
                 gpt_model: str = 'gpt-4o-mini'):
        """
        Args:
            kb: –≠–∫–∑–µ–º–ø–ª—è—Ä KnowledgeBase
            embedding_service: –°–µ—Ä–≤–∏—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embeddings
            vector_store: –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ FAISS
            db_path: –ü—É—Ç—å –∫ –ë–î
            gpt_model: –ú–æ–¥–µ–ª—å GPT –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤
        """
        self.kb = kb
        self.embedding_service = embedding_service
        self.vector_store = vector_store
        self.db_path = db_path
        self.gpt_model = gpt_model

        # –ö—ç—à –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.documents_cache = {}
        self.documents_indexed = False

        logger.info(f"‚úÖ SmartAssistant initialized with {gpt_model}")

    def index_markdown_files(self, docs_dir: str = '.') -> int:
        """
        –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ .md —Ñ–∞–π–ª—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î

        Args:
            docs_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        """
        logger.info(f"üìö Indexing .md files from {docs_dir}...")

        indexed_count = 0

        for root, dirs, files in os.walk(docs_dir):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º venv –∏ —Å–∫—Ä—ã—Ç—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            dirs[:] = [d for d in dirs if not d.startswith('.') and d != 'venv']

            for file in files:
                if not file.endswith('.md'):
                    continue

                file_path = os.path.join(root, file)

                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã
                    if len(content.strip()) < 50:
                        continue

                    # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã –Ω–∞ —á–∞–Ω–∫–∏
                    chunks = self._split_into_chunks(content, max_length=1000)

                    for i, chunk in enumerate(chunks):
                        # –°–æ–∑–¥–∞—ë–º –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                        question = f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ {file} (—á–∞—Å—Ç—å {i+1}/{len(chunks)})"
                        answer = chunk

                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
                        try:
                            self.kb.add(
                                question=question,
                                answer=answer,
                                category='documentation',
                                tags=f"md,doc,{file}",
                                source='auto_index',
                                added_by=0  # system
                            )
                            indexed_count += 1
                        except Exception as e:
                            logger.error(f"‚ùå Error indexing {file_path}: {e}")

                    logger.info(f"   ‚úÖ Indexed {file} ({len(chunks)} chunks)")

                except Exception as e:
                    logger.error(f"‚ùå Error reading {file_path}: {e}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.vector_store.save()
        self.documents_indexed = True

        logger.info(f"‚úÖ Indexed {indexed_count} document chunks")
        return indexed_count

    def _split_into_chunks(self, text: str, max_length: int = 1000) -> List[str]:
        """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        chunks = []
        lines = text.split('\n')

        current_chunk = []
        current_length = 0

        for line in lines:
            line_length = len(line)

            if current_length + line_length > max_length and current_chunk:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                chunks.append('\n'.join(current_chunk))
                current_chunk = []
                current_length = 0

            current_chunk.append(line)
            current_length += line_length + 1  # +1 –¥–ª—è \n

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunks.append('\n'.join(current_chunk))

        return chunks if chunks else [text]

    def answer_with_context(self,
                          question: str,
                          user_id: Optional[int] = None,
                          chat_history: Optional[List[Dict]] = None,
                          mode: str = 'auto') -> Tuple[str, float, List[Dict], str]:
        """
        –û—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å —É—á—ë—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏)
            chat_history: –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–æ–æ–±—â–µ–Ω–∏–π)
            mode: –†–µ–∂–∏–º –æ—Ç–≤–µ—Ç–∞ ('auto', 'strict', 'creative', 'docs')

        Returns:
            (answer, confidence, search_results, source_type)
        """
        # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        search_results = self.kb.vector_search(question, top_k=5, min_score=0.60)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞
        question_type = self._classify_question(question)

        # –í—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ—Ç–≤–µ—Ç–∞
        if mode == 'auto':
            if search_results and search_results[0]['score'] >= 0.75:
                # –•–æ—Ä–æ—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ - —Å—Ç—Ä–æ–≥–∏–π RAG
                return self._strict_rag_answer(question, search_results)
            elif question_type in ['greeting', 'smalltalk']:
                # –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ –∏–ª–∏ —Å–≤–µ—Ç—Å–∫–∞—è –±–µ—Å–µ–¥–∞ - GPT –±–µ–∑ RAG
                return self._casual_answer(question)
            elif question_type == 'procedural':
                # –ü—Ä–æ—Ü–µ–¥—É—Ä–Ω—ã–π –≤–æ–ø—Ä–æ—Å - RAG + GPT
                return self._hybrid_answer(question, search_results, chat_history)
            else:
                # –û–±—â–∏–π –≤–æ–ø—Ä–æ—Å
                return self._hybrid_answer(question, search_results, chat_history)

        elif mode == 'strict':
            # –¢–æ–ª—å–∫–æ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
            return self._strict_rag_answer(question, search_results)

        elif mode == 'creative':
            # GPT —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            return self._creative_answer(question, search_results)

        elif mode == 'docs':
            # –¢–æ–ª—å–∫–æ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            doc_results = [r for r in search_results if 'documentation' in r.get('category', '')]
            return self._strict_rag_answer(question, doc_results)

        # Fallback
        return self._hybrid_answer(question, search_results, chat_history)

    def _classify_question(self, question: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –≤–æ–ø—Ä–æ—Å–∞"""
        question_lower = question.lower().strip()

        # –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è
        greetings = ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä—ã–π –¥–µ–Ω—å', '–¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä', 'hi', 'hello']
        if any(g in question_lower for g in greetings):
            return 'greeting'

        # –°–≤–µ—Ç—Å–∫–∞—è –±–µ—Å–µ–¥–∞
        smalltalk = ['–∫–∞–∫ –¥–µ–ª–∞', '—á—Ç–æ –¥–µ–ª–∞–µ—à—å', '–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ', '—á—Ç–æ –Ω–æ–≤–æ–≥–æ']
        if any(s in question_lower for s in smalltalk):
            return 'smalltalk'

        # –ü—Ä–æ—Ü–µ–¥—É—Ä–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã (–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å)
        procedural = ['–∫–∞–∫', '—á—Ç–æ –¥–µ–ª–∞—Ç—å', '–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å', '–∫–∞–∫ –∑–∞–∫—Ä—ã—Ç—å', '–∫–∞–∫ –æ—Ç–∫—Ä—ã—Ç—å', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è']
        if any(p in question_lower for p in procedural):
            return 'procedural'

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        informational = ['—á—Ç–æ —Ç–∞–∫–æ–µ', '–∫—Ç–æ —Ç–∞–∫–æ–π', '–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '–∫–æ–≥–¥–∞']
        if any(i in question_lower for i in informational):
            return 'informational'

        return 'general'

    def _strict_rag_answer(self, question: str, results: List[Dict]) -> Tuple[str, float, List[Dict], str]:
        """–°—Ç—Ä–æ–≥–∏–π RAG - —Ç–æ–ª—å–∫–æ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        if not results:
            return "–í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É.", 0.0, [], "none"

        top = results[0]

        # –ï—Å–ª–∏ —Å–∫–æ—Ä —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π
        if top['score'] < 0.60:
            return "–í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É.", top['score'], results, "none"

        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –∏–∑ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        answer_parts = []

        for i, result in enumerate(results[:3], 1):
            if result['score'] < 0.55:
                break

            answer = result['answer']
            if len(answer) > 600:
                answer = answer[:600] + "..."

            if i == 1:
                answer_parts.append(answer)
            else:
                answer_parts.append(f"\n\nüìé –¢–∞–∫–∂–µ:\n{answer}")

        final_answer = "".join(answer_parts)

        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        sources = ', '.join([f"[{r['id']}]" for r in results[:3]])
        final_answer += f"\n\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {sources}"

        return final_answer, top['score'], results, "knowledge_base"

    def _casual_answer(self, question: str) -> Tuple[str, float, List[Dict], str]:
        """–ù–µ–ø—Ä–∏–Ω—É–∂–¥—ë–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç (–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ, —Å–≤–µ—Ç—Å–∫–∞—è –±–µ—Å–µ–¥–∞)"""
        try:
            response = openai.ChatCompletion.create(
                model=self.gpt_model,
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "–¢—ã - –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∫–ª—É–±–∞. "
                            "–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –ø–æ–∑–∏—Ç–∏–≤–Ω–æ –∏ –ø–æ-–¥—Ä—É–∂–µ—Å–∫–∏. "
                            "–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."
                        )
                    },
                    {"role": "user", "content": question}
                ],
                temperature=0.8,
                max_tokens=150
            )
            answer = response['choices'][0]['message']['content'].strip()
            return answer, 0.5, [], "casual"
        except Exception as e:
            logger.error(f"‚ùå Casual answer error: {e}")
            return "–ü—Ä–∏–≤–µ—Ç! –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?", 0.0, [], "fallback"

    def _hybrid_answer(self,
                      question: str,
                      results: List[Dict],
                      chat_history: Optional[List[Dict]] = None) -> Tuple[str, float, List[Dict], str]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –æ—Ç–≤–µ—Ç - RAG + GPT"""

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        context_parts = []

        if results:
            context_parts.append("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:")
            for i, result in enumerate(results[:3], 1):
                context_parts.append(f"\n{i}. {result['answer'][:400]}")

        context = "\n".join(context_parts) if context_parts else "–ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ."

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é
        history_text = ""
        if chat_history:
            history_text = "\n\n–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è:\n"
            for msg in chat_history[-5:]:
                history_text += f"- {msg.get('from', 'User')}: {msg.get('text', '')[:100]}\n"

        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        system_prompt = """–¢—ã - —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∫–ª—É–±–∞.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
1. –û—Ç–≤–µ—á–∞–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
2. –ï—Å–ª–∏ –≤ –±–∞–∑–µ –Ω–µ—Ç —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–π —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è, –Ω–æ —á–µ—Å—Ç–Ω–æ —É–∫–∞–∂–∏ —ç—Ç–æ
3. –ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º
4. –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å - —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏

–ü—Ä–∞–≤–∏–ª–∞:
- –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã –æ –∫–ª—É–±–µ, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
- –î–ª—è –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–∞–≤–∞–π –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
- –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –Ω–æ –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —Ç–æ–Ω"""

        user_prompt = f"""–í–æ–ø—Ä–æ—Å: {question}

{context}
{history_text}

–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."""

        try:
            response = openai.ChatCompletion.create(
                model=self.gpt_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=500
            )

            answer = response['choices'][0]['message']['content'].strip()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º confidence –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–ª–∏—á–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            confidence = results[0]['score'] if results else 0.4

            return answer, confidence, results, "hybrid"

        except Exception as e:
            logger.error(f"‚ùå Hybrid answer error: {e}")

            # Fallback –Ω–∞ —Å—Ç—Ä–æ–≥–∏–π RAG
            if results:
                return self._strict_rag_answer(question, results)

            return "–ù–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å.", 0.0, [], "error"

    def _creative_answer(self, question: str, results: List[Dict]) -> Tuple[str, float, List[Dict], str]:
        """–ö—Ä–µ–∞—Ç–∏–≤–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = ""
        if results:
            context = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {results[0]['answer'][:200]}"

        system_prompt = """–¢—ã - –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∫–ª—É–±–∞.
–û—Ç–≤–µ—á–∞–π –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏, –æ–±—ä—è—Å–Ω—è–π –ø–æ–Ω—è—Ç–Ω–æ.
–ú–æ–∂–µ—à—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–Ω–∞–ª–æ–≥–∏–∏ –∏ –º–µ—Ç–∞—Ñ–æ—Ä—ã."""

        try:
            response = openai.ChatCompletion.create(
                model=self.gpt_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"{context}\n\n–í–æ–ø—Ä–æ—Å: {question}"}
                ],
                temperature=0.7,
                max_tokens=600
            )

            answer = response['choices'][0]['message']['content'].strip()
            return answer, 0.6, results, "creative"

        except Exception as e:
            logger.error(f"‚ùå Creative answer error: {e}")
            return "–ù–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å.", 0.0, [], "error"

    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
        return {
            'documents_indexed': self.documents_indexed,
            'vector_store_size': self.vector_store.stats()['total_vectors'],
            'kb_size': self._get_kb_size(),
            'model': self.gpt_model
        }

    def _get_kb_size(self) -> int:
        """–†–∞–∑–º–µ—Ä –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('SELECT COUNT(*) FROM knowledge')
            count = cursor.fetchone()[0]
            conn.close()
            return count
        except:
            return 0


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == '__main__':
    print("SmartAssistant module - use via bot.py")
